{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2026-02-20T19:57:12.136906Z",
     "start_time": "2026-02-20T19:56:54.366806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from torch.distributed.tensor.parallel import loss_parallel\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "\n",
    "from prepare_data import ds_train, ds_test, Tkn, dl_train, dl_test\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MODEL WEIGHTS\n",
    "DIM_IMAGE_EMBEDDING = 512\n",
    "DIM_ATTENTION = 256\n",
    "D_PIC = 400\n",
    "N_HEAD_ATTENTION = 16\n",
    "\n",
    "# PICTURE\n",
    "N_CONTEXT = 768  # num of patched in picture\n",
    "W = int(os.getenv(\"W\"))\n",
    "H = int(os.getenv(\"H\"))\n",
    "KERNEL = 20\n",
    "\n",
    "# TEXT PARAMETERS\n",
    "MAX_LEN_OF_TEXT_CONTEXT = 188 + 1  # 187 - max len of tokens sequence (mean text in utf-8). 1 - special token for start of sequence\n",
    "DIM_TEXT_EMBEDDING = 128\n",
    "DIM_TEXT_ATTENTION = 128\n",
    "N_HEAD_LATEX_ATTENTION = 32\n",
    "VOCAB_SIZE = 303\n",
    "\n",
    "# CROSS-MECHANISM PARAMETERS\n",
    "DIM_CROSS_EMBEDDING = 512\n",
    "N_HEAD_CROSS_ATTENTION = 16\n",
    "DIM_CROSS_ATTENTION = 256\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def precompute_theta_2D_per_frequencies(head_size: int, theta=10000.0):\n",
    "    m_x = torch.arange(0, W // KERNEL).repeat(H // KERNEL).to(device)\n",
    "    m_y = torch.arange(0, H // KERNEL).repeat_interleave(W // KERNEL).to(device)\n",
    "    theta_pairs_numerator = torch.arange(0, head_size // 2, 2).float()\n",
    "    theta = 1.0 / (theta ** (2 * theta_pairs_numerator / head_size)).to(device)\n",
    "    freqs_x = torch.outer(m_x, theta).float()\n",
    "    freqs_y = torch.outer(m_y, theta).float()\n",
    "    freqs_complex_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "    freqs_complex_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "    return freqs_complex_x, freqs_complex_y  # (T, head_size // 2)\n",
    "\n",
    "    # theta_pairs_numerator = torch.arange(0, head_size, 2).float() # i of each pair in emb\n",
    "    # theta = 1.0 / (theta ** (2 * theta_pairs_numerator / head_size)).to(device)\n",
    "    # m = torch.arange(m, device=device)\n",
    "    # freqs = torch.outer(m, theta).float()\n",
    "    # freqs_complex = torch.polar(torch.ones_like(freqs), freqs) # convert to complex space based on Euler's formula e ^ (it)\n",
    "    # return freqs_complex #(T, head_size // 2)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_rotary_2D_embeddings(x: torch.Tensor, freqs_complex_x: torch.Tensor, freqs_complex_y: torch.Tensor):\n",
    "    v_x, v_y = torch.chunk(x, 2, dim=-1)  # each: (B, T, head_size // 2)\n",
    "    v_x_complex = torch.view_as_complex(v_x.float().reshape(*v_x.shape[:-1], -1, 2))  # (B, T, head_size // 4)\n",
    "    v_y_complex = torch.view_as_complex(v_y.float().reshape(*v_y.shape[:-1], -1, 2))  # (B, T, head_size // 4)\n",
    "    freqs_complex_x = freqs_complex_x.unsqueeze(0)\n",
    "    freqs_complex_y = freqs_complex_y.unsqueeze(0)\n",
    "    v_x_rotated = v_x_complex * freqs_complex_x\n",
    "    v_y_rotated = v_y_complex * freqs_complex_y\n",
    "    v_x_out = torch.view_as_real(v_x_rotated)  # (B, T, head_size // 4, 2)\n",
    "    v_y_out = torch.view_as_real(v_y_rotated)  # (B, T, head_size // 4, 2)\n",
    "    v_x_out = v_x_out.reshape(*v_x.shape)\n",
    "    v_y_out = v_y_out.reshape(*v_y.shape)\n",
    "    x_out = torch.cat([v_x_out, v_y_out], dim=-1)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "    # x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2)) # (B, T, head_size // 2)\n",
    "    # freqs_complex = freqs_complex.unsqueeze(0) # (1, T, head_size // 2)\n",
    "    # x_rotated = x_complex * freqs_complex\n",
    "    # x_out = torch.view_as_real(x_rotated) #(B, T, head_size // 2, 2)\n",
    "    # x_out = x_out.reshape(*x.shape)\n",
    "    # return x_out.type_as(x).to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def precompute_theta_1D_per_frequencies(head_size: int, theta=10000.0, m=MAX_LEN_OF_TEXT_CONTEXT):\n",
    "    theta_pairs_numerator = torch.arange(0, head_size, 2).float()  # i of each pair in emb\n",
    "    theta = 1.0 / (theta ** (2 * theta_pairs_numerator / head_size)).to(device)\n",
    "    m = torch.arange(m, device=device)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs),\n",
    "                                freqs)  # convert to complex space based on Euler's formula e ^ (it)\n",
    "    return freqs_complex  # (T, head_size // 2)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_rotary_1D_embeddings(x: torch.Tensor, freqs_complex):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))  # (B, T, head_size // 2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0)  # (1, T, head_size // 2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    x_out = torch.view_as_real(x_rotated)  # (B, T, head_size // 2, 2)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size, freqs_complex_x: torch.Tensor,\n",
    "                 freqs_complex_y: torch.Tensor):  # patch dim - dim of embedding space, head_size - dim of head space\n",
    "        super().__init__()\n",
    "        self.freqs_complex_x, self.freqs_complex_y = freqs_complex_x, freqs_complex_y\n",
    "        self.key = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)\n",
    "        self.query = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)\n",
    "        self.value = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # (batch, count of elements, count of features)\n",
    "        k = self.key(x)  # (B, T, head_size)\n",
    "        k = apply_rotary_2D_embeddings(k, self.freqs_complex_x, self.freqs_complex_y)\n",
    "\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        q = apply_rotary_2D_embeddings(q, self.freqs_complex_x, self.freqs_complex_y)\n",
    "\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        wei = (q @ k.transpose(-2, -1) / (C ** 0.5))  # (B, T, T)\n",
    "        softwei = F.softmax(wei, dim=-1)\n",
    "        out = softwei @ v\n",
    "        return out  # (B, T, head_size)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freqs_complex_x, self.freqs_complex_y = precompute_theta_2D_per_frequencies(DIM_ATTENTION)\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(DIM_ATTENTION, self.freqs_complex_x, self.freqs_complex_y) for i in range(N_HEAD_ATTENTION)])\n",
    "        self.proj = nn.Linear(N_HEAD_ATTENTION * DIM_ATTENTION, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out  # (B, T, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_in, embedding_out=None):\n",
    "        if embedding_out == None:\n",
    "            embedding_out = embedding_in\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_in, embedding_in * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_in * 4, embedding_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ResViTBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention()\n",
    "        self.mlp = FeedForward(DIM_IMAGE_EMBEDDING)\n",
    "        self.ln1 = nn.LayerNorm(DIM_IMAGE_EMBEDDING)\n",
    "        self.ln2 = nn.LayerNorm(DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x  # (B, T, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "\n",
    "class AttentionHead_Latex_withMask(nn.Module):\n",
    "    def __init__(self, head_size,\n",
    "                 freqs_complex: torch.Tensor):  # patch dim - dim of embedding space, head_size - dim of head space\n",
    "        super().__init__()\n",
    "        self.freqs_complex = freqs_complex\n",
    "        self.key = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False)\n",
    "        self.query = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False)\n",
    "        self.value = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x_text_emb, x_latex_mask):\n",
    "        B, T, C = x_text_emb.shape  # (batch, count of elements, count of features)\n",
    "        k = self.key(x_text_emb)  # (B, T, head_size)\n",
    "        k = apply_rotary_1D_embeddings(k, self.freqs_complex)\n",
    "\n",
    "        q = self.query(x_text_emb)  # (B, T, head_size)\n",
    "        q = apply_rotary_1D_embeddings(q, self.freqs_complex)\n",
    "\n",
    "        v = self.value(x_text_emb)  # (B, T, head_size)\n",
    "        wei = (q @ k.transpose(-2, -1) / (C ** 0.5))  # (B, T, T)\n",
    "        trill = torch.tril(torch.ones((T, T))).to(device)\n",
    "        wei = wei.masked_fill(trill == 0, float(\"-inf\"))  # trial mask\n",
    "        wei = wei.masked_fill(x_latex_mask.unsqueeze(1) == 0, float(\"-inf\"))  # padding mask (by rows)\n",
    "        softwei = F.softmax(wei, dim=-1)\n",
    "        out = softwei @ v\n",
    "        return out  # (B, T, head_size)\n",
    "\n",
    "\n",
    "class MultiHeadAttention_LaTeX(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freqs_complex = precompute_theta_1D_per_frequencies(DIM_TEXT_ATTENTION)\n",
    "        self.heads = nn.ModuleList([AttentionHead_Latex_withMask(DIM_TEXT_ATTENTION, self.freqs_complex) for i in\n",
    "                                    range(N_HEAD_LATEX_ATTENTION)])\n",
    "        self.proj = nn.Linear(N_HEAD_LATEX_ATTENTION * DIM_TEXT_ATTENTION, DIM_TEXT_EMBEDDING)\n",
    "\n",
    "    def forward(self, x_text_emb, x_latex_mask):\n",
    "        out = torch.cat([h(x_text_emb, x_latex_mask) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out  # (B, T, DIM_TEXT_EMBEDDING)\n",
    "\n",
    "\n",
    "class ResLaTeXBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention_LaTeX()\n",
    "        self.mlp = FeedForward(DIM_TEXT_EMBEDDING)\n",
    "        self.ln1 = nn.LayerNorm(DIM_TEXT_EMBEDDING)\n",
    "        self.ln2 = nn.LayerNorm(DIM_TEXT_EMBEDDING)\n",
    "\n",
    "    def forward(self, X):  # x_text_emb, x_latex_mask\n",
    "        x_text_emb, x_latex_mask = X\n",
    "        x = x_text_emb + self.att(self.ln1(x_text_emb), x_latex_mask)\n",
    "        x = x_text_emb + self.mlp(self.ln2(x))\n",
    "        return x  # (B, T, DIM_TEXT_EMBEDDING)\n",
    "\n",
    "\n",
    "class Cross_AttentionHead_withMask(nn.Module):\n",
    "    def __init__(self, head_size, freqs_complex_latex, freqs_complex_image_x,\n",
    "                 freqs_complex_image_y):  # patch dim - dim of embedding space, head_size - dim of head space\n",
    "        super().__init__()\n",
    "        self.freqs_complex = freqs_complex_latex\n",
    "        self.freqs_complex_image_x = freqs_complex_image_x\n",
    "        self.freqs_complex_image_y = freqs_complex_image_y\n",
    "\n",
    "        self.key = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)  # (B, T1, head_size)\n",
    "        self.query = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False)  # (B, T2, head_size)\n",
    "        self.value = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)  # (B, T1, head_size)\n",
    "\n",
    "    def forward(self, x_image, x_text_emb, x_latex_mask):\n",
    "        B, T, C = x_image.shape  # (batch, count of elements, count of features)\n",
    "\n",
    "        k = self.key(x_image)  # (B, T_k, head_size)\n",
    "        k = apply_rotary_2D_embeddings(k, self.freqs_complex_image_x, self.freqs_complex_image_y)\n",
    "\n",
    "        q = self.query(x_text_emb)  # (B, T_q, head_size)\n",
    "        q = apply_rotary_1D_embeddings(q, self.freqs_complex)\n",
    "\n",
    "        v = self.value(x_image)  # (B, T_k, head_size)\n",
    "        wei = (q @ k.transpose(-2, -1) / (C ** 0.5))  # (B, T_q, T_k)\n",
    "\n",
    "        # if torch.isinf(wei).all(dim=-1).any():\n",
    "        #     print(\"FULLY MASKED ROW FOUND\")\n",
    "        softwei = F.softmax(wei, dim=-1)\n",
    "        out = softwei @ v\n",
    "        return out  # (B, T_q, head_size)\n",
    "\n",
    "\n",
    "class Cross_MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freqs_complex_latex = precompute_theta_1D_per_frequencies(DIM_CROSS_ATTENTION)\n",
    "        self.freqs_complex_image_x, self.freqs_complex_image_y = precompute_theta_2D_per_frequencies(\n",
    "            DIM_CROSS_ATTENTION)\n",
    "\n",
    "        self.heads = nn.ModuleList([Cross_AttentionHead_withMask(DIM_CROSS_ATTENTION, self.freqs_complex_latex,\n",
    "                                                                 self.freqs_complex_image_x, self.freqs_complex_image_y)\n",
    "                                    for i in range(N_HEAD_CROSS_ATTENTION)])\n",
    "        self.proj = nn.Linear(N_HEAD_CROSS_ATTENTION * DIM_CROSS_ATTENTION, DIM_CROSS_EMBEDDING)\n",
    "\n",
    "    def forward(self, x_image, x_latex, x_latex_mask):\n",
    "        out = torch.cat([h(x_image, x_latex, x_latex_mask) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out  # (B, T_latex, DIM_CROSS_EMBEDDING)\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = Cross_MultiHeadAttention()\n",
    "        self.mlp = FeedForward(DIM_CROSS_EMBEDDING)\n",
    "        self.ln1 = nn.LayerNorm(DIM_IMAGE_EMBEDDING)\n",
    "        self.ln2 = nn.LayerNorm(DIM_TEXT_EMBEDDING)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x_latex, x_image, x_latex_mask = X\n",
    "        # x_latex (B, 190, DIM_TEXT_EMBEDDING)\n",
    "        # x_image (B, T=768, DIM_IMAGE, DIM_IMAGE_EMBEDDING)\n",
    "        x = self.att(self.ln1(x_image), self.ln2(x_latex), x_latex_mask)  # (B, T_latex=190, DIM_CROSS_EMBEDDING)\n",
    "        x = x + self.mlp(x)\n",
    "        return x  # (B, T_latex, DIM_CROSS_EMBEDDING)\n",
    "\n",
    "\n",
    "class FormulaAI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_to_emb = nn.Linear(D_PIC, DIM_IMAGE_EMBEDDING)\n",
    "        self.VIT_transformers = nn.Sequential(\n",
    "            ResViTBlock(),\n",
    "            ResViTBlock()\n",
    "        )\n",
    "        self.latex_transformers = nn.Sequential(\n",
    "            ResLaTeXBlock()\n",
    "        )\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE + 1, DIM_TEXT_EMBEDDING)\n",
    "        self.cross_attention = nn.Sequential(\n",
    "            CrossAttentionBlock()\n",
    "        )\n",
    "        self.final_mlp = FeedForward(DIM_CROSS_EMBEDDING, VOCAB_SIZE + 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x_image, x_latex, y = X\n",
    "        x_image = x_image.to(device)\n",
    "        x_latex = x_latex.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        x_image_emb = self.image_to_emb(x_image)\n",
    "        x_image_VIT = self.VIT_transformers(x_image_emb)  # (B, T_image, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "        # x_latex:\n",
    "        # [[301, 178, 204, 303, 303, .., 303]]     [302] - not here, because it's x_latex, not target\n",
    "\n",
    "        x_latex_mask = Tkn.mask_padding(x_latex).to(device)\n",
    "        x_text_emb = self.token_embedding_table(x_latex)  # (B, T, DIM_TEXT_EMBEDDING)\n",
    "        x_processed_latex = self.latex_transformers((x_text_emb, x_latex_mask))  # (B, T_latex, DIM_TEXT_EMBEDDING)\n",
    "        logits = self.cross_attention(\n",
    "            (x_processed_latex, x_image_VIT, x_latex_mask))  # (B, T_latex, DIM_CROSS_EMBEDDING)\n",
    "        logits = self.final_mlp(logits)  # (B, T_latex, vocab_size)\n",
    "\n",
    "        B, T_latex, voc = logits.shape\n",
    "        logits = logits.view(B * T_latex, voc)\n",
    "        targets = y.view(B * T_latex)\n",
    "        loss = F.cross_entropy(logits, targets, ignore_index=Tkn.padding)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "# ---- TRAIN ----\n",
    "model = FormulaAI().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "EPOCHS = 100\n",
    "\n",
    "losses = []\n",
    "cnt = 0\n",
    "for epoch in trange(EPOCHS):\n",
    "    for sample in dl_train:\n",
    "        logits, loss = model(sample)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        cnt += 1\n",
    "        if cnt % 100 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "c65511686ccdc0a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загрузка датасета MathWriting-human...\n",
      "датасет MathWriting-human загружен успешно\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4653e6a5894b9fc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
