{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-20T16:26:39.487090Z",
     "start_time": "2026-02-20T16:26:33.270751Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from torch.distributed.tensor.parallel import loss_parallel\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from prepare_data import ds_train, ds_test, Tkn, dl_train, dl_test\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MODEL WEIGHTS\n",
    "DIM_IMAGE_EMBEDDING = 256\n",
    "DIM_ATTENTION = 32\n",
    "D_PIC = 400\n",
    "N_HEAD_ATTENTION = 8\n",
    "\n",
    "# PICTURE\n",
    "N_CONTEXT = 768 # num of patched in picture\n",
    "W = int(os.getenv(\"W\"))\n",
    "H = int(os.getenv(\"H\"))\n",
    "KERNEL = 20\n",
    "\n",
    "#TEXT PARAMETERS\n",
    "MAX_LEN_OF_TEXT_CONTEXT = 188 + 1 # 187 - max len of tokens sequence (mean text in utf-8). 1 - special token for start of sequence\n",
    "DIM_TEXT_EMBEDDING = 128\n",
    "DIM_TEXT_ATTENTION = 32\n",
    "N_HEAD_LATEX_ATTENTION = 8\n",
    "VOCAB_SIZE = 303\n",
    "\n",
    "\n",
    "# CROSS-MECHANISM PARAMETERS\n",
    "DIM_CROSS_EMBEDDING = 128\n",
    "N_HEAD_CROSS_ATTENTION = 8\n",
    "DIM_CROSS_ATTENTION = 32\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def precompute_theta_2D_per_frequencies(head_size: int, theta = 10000.0):\n",
    "    m_x = torch.arange(0, W // KERNEL).repeat(H // KERNEL)\n",
    "    m_y = torch.arange(0, H // KERNEL).repeat_interleave(W // KERNEL)\n",
    "    theta_pairs_numerator = torch.arange(0, head_size // 2, 2).float()\n",
    "    theta = 1.0 / (theta ** (2 * theta_pairs_numerator / head_size)).to(device)\n",
    "    freqs_x = torch.outer(m_x, theta).float()\n",
    "    freqs_y = torch.outer(m_y, theta).float()\n",
    "    freqs_complex_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "    freqs_complex_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "    return freqs_complex_x, freqs_complex_y #(T, head_size // 2)\n",
    "\n",
    "    # theta_pairs_numerator = torch.arange(0, head_size, 2).float() # i of each pair in emb\n",
    "    # theta = 1.0 / (theta ** (2 * theta_pairs_numerator / head_size)).to(device)\n",
    "    # m = torch.arange(m, device=device)\n",
    "    # freqs = torch.outer(m, theta).float()\n",
    "    # freqs_complex = torch.polar(torch.ones_like(freqs), freqs) # convert to complex space based on Euler's formula e ^ (it)\n",
    "    # return freqs_complex #(T, head_size // 2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_rotary_2D_embeddings(x: torch.Tensor, freqs_complex_x: torch.Tensor, freqs_complex_y: torch.Tensor):\n",
    "    v_x, v_y = torch.chunk(x, 2, dim=-1) # each: (B, T, head_size // 2)\n",
    "    v_x_complex = torch.view_as_complex(v_x.float().reshape(*v_x.shape[:-1], -1, 2)) # (B, T, head_size // 4)\n",
    "    v_y_complex = torch.view_as_complex(v_y.float().reshape(*v_y.shape[:-1], -1, 2)) # (B, T, head_size // 4)\n",
    "    freqs_complex_x = freqs_complex_x.unsqueeze(0)\n",
    "    freqs_complex_y = freqs_complex_y.unsqueeze(0)\n",
    "    v_x_rotated = v_x_complex * freqs_complex_x\n",
    "    v_y_rotated = v_y_complex * freqs_complex_y\n",
    "    v_x_out = torch.view_as_real(v_x_rotated) # (B, T, head_size // 4, 2)\n",
    "    v_y_out = torch.view_as_real(v_y_rotated) # (B, T, head_size // 4, 2)\n",
    "    v_x_out = v_x_out.reshape(*v_x.shape)\n",
    "    v_y_out = v_y_out.reshape(*v_y.shape)\n",
    "    x_out = torch.cat([v_x_out, v_y_out], dim=-1)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "    # x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2)) # (B, T, head_size // 2)\n",
    "    # freqs_complex = freqs_complex.unsqueeze(0) # (1, T, head_size // 2)\n",
    "    # x_rotated = x_complex * freqs_complex\n",
    "    # x_out = torch.view_as_real(x_rotated) #(B, T, head_size // 2, 2)\n",
    "    # x_out = x_out.reshape(*x.shape)\n",
    "    # return x_out.type_as(x).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def precompute_theta_1D_per_frequencies(head_size: int, theta = 10000.0, m = MAX_LEN_OF_TEXT_CONTEXT):\n",
    "    theta_pairs_numerator = torch.arange(0, head_size, 2).float() # i of each pair in emb\n",
    "    theta = 1.0 / (theta ** (2 * theta_pairs_numerator / head_size)).to(device)\n",
    "    m = torch.arange(m, device=device)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs) # convert to complex space based on Euler's formula e ^ (it)\n",
    "    return freqs_complex #(T, head_size // 2)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_rotary_1D_embeddings(x: torch.Tensor, freqs_complex):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2)) # (B, T, head_size // 2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0) # (1, T, head_size // 2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    x_out = torch.view_as_real(x_rotated) #(B, T, head_size // 2, 2)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size, freqs_complex_x: torch.Tensor, freqs_complex_y: torch.Tensor): # patch dim - dim of embedding space, head_size - dim of head space\n",
    "        super().__init__()\n",
    "        self.freqs_complex_x, self.freqs_complex_y = freqs_complex_x, freqs_complex_y\n",
    "        self.key = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)\n",
    "        self.query = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)\n",
    "        self.value = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape #(batch, count of elements, count of features)\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        k = apply_rotary_2D_embeddings(k, self.freqs_complex_x, self.freqs_complex_y)\n",
    "\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        q = apply_rotary_2D_embeddings(q, self.freqs_complex_x, self.freqs_complex_y)\n",
    "\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        wei = (q @ k.transpose(-2, -1) / (C ** 0.5)) # (B, T, T)\n",
    "        softwei = F.softmax(wei, dim=-1)\n",
    "        out = softwei @ v\n",
    "        return out # (B, T, head_size)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freqs_complex_x, self.freqs_complex_y = precompute_theta_2D_per_frequencies(DIM_ATTENTION)\n",
    "        self.heads = nn.ModuleList([AttentionHead(DIM_ATTENTION, self.freqs_complex_x, self.freqs_complex_y) for i in range(N_HEAD_ATTENTION)])\n",
    "        self.proj = nn.Linear(N_HEAD_ATTENTION * DIM_ATTENTION, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out # (B, T, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_in, embedding_out=None):\n",
    "        if embedding_out == None:\n",
    "            embedding_out = embedding_in\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_in, embedding_in * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_in * 4, embedding_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ResViTBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention()\n",
    "        self.mlp = FeedForward(DIM_IMAGE_EMBEDDING)\n",
    "        self.ln1 = nn.LayerNorm(DIM_IMAGE_EMBEDDING)\n",
    "        self.ln2 = nn.LayerNorm(DIM_IMAGE_EMBEDDING)\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x # (B, T, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "class AttentionHead_Latex_withMask(nn.Module):\n",
    "    def __init__(self, head_size, freqs_complex: torch.Tensor): # patch dim - dim of embedding space, head_size - dim of head space\n",
    "        super().__init__()\n",
    "        self.freqs_complex = freqs_complex\n",
    "        self.key = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False)\n",
    "        self.query = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False)\n",
    "        self.value = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False)\n",
    "    def forward(self, x_text_emb, x_latex_mask):\n",
    "        B, T, C = x_text_emb.shape #(batch, count of elements, count of features)\n",
    "        k = self.key(x_text_emb) # (B, T, head_size)\n",
    "        k = apply_rotary_1D_embeddings(k, self.freqs_complex)\n",
    "\n",
    "        q = self.query(x_text_emb) # (B, T, head_size)\n",
    "        q = apply_rotary_1D_embeddings(q, self.freqs_complex)\n",
    "\n",
    "        v = self.value(x_text_emb) # (B, T, head_size)\n",
    "        wei = (q @ k.transpose(-2, -1) / (C ** 0.5)) # (B, T, T)\n",
    "        trill = torch.tril(torch.ones((T, T)))\n",
    "        wei = wei.masked_fill(trill == 0, float(\"-inf\")) # trial mask\n",
    "        wei = wei.masked_fill(x_latex_mask.unsqueeze(1) == 0, float(\"-inf\")) # padding mask (by rows)\n",
    "        softwei = F.softmax(wei, dim=-1)\n",
    "        out = softwei @ v\n",
    "        return out # (B, T, head_size)\n",
    "\n",
    "class MultiHeadAttention_LaTeX(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freqs_complex = precompute_theta_1D_per_frequencies(DIM_TEXT_ATTENTION)\n",
    "        self.heads = nn.ModuleList([AttentionHead_Latex_withMask(DIM_TEXT_ATTENTION, self.freqs_complex) for i in range(N_HEAD_LATEX_ATTENTION)])\n",
    "        self.proj = nn.Linear(N_HEAD_LATEX_ATTENTION * DIM_TEXT_ATTENTION, DIM_TEXT_EMBEDDING)\n",
    "\n",
    "    def forward(self, x_text_emb, x_latex_mask):\n",
    "        out = torch.cat([h(x_text_emb, x_latex_mask) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out # (B, T, DIM_TEXT_EMBEDDING)\n",
    "\n",
    "class ResLaTeXBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention_LaTeX()\n",
    "        self.mlp = FeedForward(DIM_TEXT_EMBEDDING)\n",
    "        self.ln1 = nn.LayerNorm(DIM_TEXT_EMBEDDING)\n",
    "        self.ln2 = nn.LayerNorm(DIM_TEXT_EMBEDDING)\n",
    "    def forward(self, X): #x_text_emb, x_latex_mask\n",
    "        x_text_emb, x_latex_mask = X\n",
    "        x = x_text_emb + self.att(self.ln1(x_text_emb), x_latex_mask)\n",
    "        x = x_text_emb + self.mlp(self.ln2(x_text_emb))\n",
    "        return x # (B, T, DIM_TEXT_EMBEDDING)\n",
    "\n",
    "\n",
    "class Cross_AttentionHead_withMask(nn.Module):\n",
    "    def __init__(self, head_size, freqs_complex_latex, freqs_complex_image_x, freqs_complex_image_y): # patch dim - dim of embedding space, head_size - dim of head space\n",
    "        super().__init__()\n",
    "        self.freqs_complex = freqs_complex_latex\n",
    "        self.freqs_complex_image_x = freqs_complex_image_x\n",
    "        self.freqs_complex_image_y = freqs_complex_image_y\n",
    "\n",
    "        self.key = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False) # (B, T1, head_size)\n",
    "        self.query = nn.Linear(DIM_TEXT_EMBEDDING, head_size, bias=False) # (B, T2, head_size)\n",
    "        self.value = nn.Linear(DIM_IMAGE_EMBEDDING, head_size, bias=False) # (B, T1, head_size)\n",
    "    def forward(self, x_image, x_text_emb, x_latex_mask):\n",
    "        B, T, C = x_image.shape #(batch, count of elements, count of features)\n",
    "\n",
    "        k = self.key(x_image) # (B, T_k, head_size)\n",
    "        k = apply_rotary_2D_embeddings(k, self.freqs_complex_image_x, self.freqs_complex_image_y)\n",
    "\n",
    "        q = self.query(x_text_emb) # (B, T_q, head_size)\n",
    "        q = apply_rotary_1D_embeddings(q, self.freqs_complex)\n",
    "\n",
    "        v = self.value(x_image) # (B, T_k, head_size)\n",
    "        wei = (q @ k.transpose(-2, -1) / (C ** 0.5)) # (B, T_q, T_k)\n",
    "        wei = wei.masked_fill(x_latex_mask.unsqueeze(-1) == 0, float(\"-inf\")) # padding mask (by columns)\n",
    "        softwei = F.softmax(wei, dim=-1)\n",
    "        out = softwei @ v\n",
    "        return out # (B, T_q, head_size)\n",
    "\n",
    "\n",
    "class Cross_MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freqs_complex_latex = precompute_theta_1D_per_frequencies(DIM_CROSS_ATTENTION)\n",
    "        self.freqs_complex_image_x, self.freqs_complex_image_y = precompute_theta_2D_per_frequencies(DIM_CROSS_ATTENTION)\n",
    "\n",
    "        self.heads = nn.ModuleList([Cross_AttentionHead_withMask(DIM_CROSS_ATTENTION, self.freqs_complex_latex, self.freqs_complex_image_x, self.freqs_complex_image_y) for i in range(N_HEAD_CROSS_ATTENTION)])\n",
    "        self.proj = nn.Linear(N_HEAD_CROSS_ATTENTION * DIM_CROSS_ATTENTION, DIM_CROSS_EMBEDDING)\n",
    "\n",
    "    def forward(self, x_image, x_latex, x_latex_mask):\n",
    "        out = torch.cat([h(x_image, x_latex, x_latex_mask) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out # (B, T_latex, DIM_CROSS_EMBEDDING)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = Cross_MultiHeadAttention()\n",
    "        self.mlp = FeedForward(DIM_CROSS_EMBEDDING)\n",
    "        self.ln1 = nn.LayerNorm(DIM_IMAGE_EMBEDDING)\n",
    "        self.ln2 = nn.LayerNorm(DIM_TEXT_EMBEDDING)\n",
    "    def forward(self, X):\n",
    "        x_latex, x_image, x_latex_mask = X\n",
    "        # x_latex (B, 190, DIM_TEXT_EMBEDDING)\n",
    "        # x_image (B, T=768, DIM_IMAGE, DIM_IMAGE_EMBEDDING)\n",
    "        x = self.att(self.ln1(x_image), self.ln2(x_latex), x_latex_mask) # (B, T_latex=190, DIM_CROSS_EMBEDDING)\n",
    "        x = x + self.mlp(x)\n",
    "        return x # (B, T_latex, DIM_CROSS_EMBEDDING)\n",
    "\n",
    "\n",
    "class FormulaAI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_emb = nn.Linear(D_PIC, DIM_IMAGE_EMBEDDING)\n",
    "        self.VIT_transformers = nn.Sequential(\n",
    "            ResViTBlock(),\n",
    "        )\n",
    "        self.latex_transformers = nn.Sequential(\n",
    "            ResLaTeXBlock(),\n",
    "        )\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE + 1, DIM_TEXT_EMBEDDING)\n",
    "        self.cross_attention = nn.Sequential(\n",
    "            CrossAttentionBlock(),\n",
    "        )\n",
    "        self.final_mlp = FeedForward(DIM_CROSS_EMBEDDING, VOCAB_SIZE + 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x_image, x_latex, y = X\n",
    "        x_image_emb = self.image_emb(x_image)\n",
    "        x_image_VIT = self.VIT_transformers(x_image_emb) # (B, T, DIM_IMAGE_EMBEDDING)\n",
    "\n",
    "        # x_latex:\n",
    "        # [[301, 178, 204, 303, 303, .., 303]]     [302] - not here, because it's x_latex, not target\n",
    "\n",
    "        x_latex_mask = Tkn.mask_padding(x_latex)\n",
    "        x_text_emb = self.token_embedding_table(x_latex) #(B, T, DIM_TEXT_EMBEDDING)\n",
    "        x_processed_latex = self.latex_transformers((x_text_emb, x_latex_mask)) # (B, T, DIM_TEXT_EMBEDDING)\n",
    "\n",
    "        logits = self.cross_attention((x_processed_latex, x_image_VIT, x_latex_mask)) # (B, T_latex, DIM_CROSS_EMBEDDING)\n",
    "        logits = self.final_mlp(logits) # (B, T_latex, vocab_size)\n",
    "\n",
    "        B, T_latex, voc = logits.shape\n",
    "        logits = logits.view(B*T_latex, voc)\n",
    "        targets = y.view(B*T_latex)\n",
    "\n",
    "        print(logits.shape)\n",
    "        print(targets.shape)\n",
    "        print(logits[0])\n",
    "        print(targets)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "# ---- TRAIN ----\n",
    "logits, loss = [], []\n",
    "model = FormulaAI()\n",
    "for sample in dl_train:\n",
    "    logits, loss = model(sample)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "загрузка датасета MathWriting-human...\n",
      "датасет MathWriting-human загружен успешно\n",
      "torch.Size([3024, 304])\n",
      "torch.Size([3024])\n",
      "tensor([ 0.0568,  0.0527, -0.0047,  0.1016, -0.0044, -0.1002,  0.0097,  0.1305,\n",
      "        -0.0187,  0.0805,  0.0914,  0.1513, -0.1250, -0.0236, -0.0039, -0.0921,\n",
      "         0.1920,  0.0191,  0.0927, -0.0140, -0.0955, -0.0441, -0.2200, -0.0281,\n",
      "         0.1040,  0.0054,  0.2076, -0.0933,  0.0078, -0.0574, -0.1126, -0.0091,\n",
      "         0.0199,  0.1791,  0.1791,  0.0226,  0.0629, -0.0807,  0.1176, -0.1186,\n",
      "        -0.0467, -0.1763,  0.0742,  0.0677, -0.2407, -0.1005, -0.0190, -0.2339,\n",
      "        -0.0102,  0.2296,  0.0881,  0.0957,  0.0021, -0.0210,  0.0252,  0.1204,\n",
      "         0.0298, -0.0197,  0.0828,  0.0379, -0.1491, -0.0054,  0.1325, -0.2118,\n",
      "        -0.0454, -0.1714,  0.0300, -0.0678,  0.0393,  0.0142,  0.2147,  0.0111,\n",
      "         0.0071, -0.0192,  0.0630,  0.1541, -0.1463, -0.0942,  0.0424,  0.0969,\n",
      "         0.2828,  0.0889, -0.0902, -0.1058, -0.1485,  0.1900, -0.1628, -0.1334,\n",
      "         0.0782, -0.0818, -0.1640,  0.1058,  0.2626, -0.1801, -0.0053, -0.1424,\n",
      "         0.0841, -0.0972,  0.0491, -0.1235,  0.0460,  0.0569, -0.0710,  0.0740,\n",
      "        -0.0723, -0.0634,  0.0341,  0.1681,  0.0835, -0.1144,  0.0513,  0.1824,\n",
      "        -0.1210,  0.1479, -0.1881, -0.2145,  0.1560,  0.0480,  0.1702, -0.0089,\n",
      "         0.1015, -0.0041,  0.0402, -0.0377, -0.1729, -0.1943, -0.0128,  0.0016,\n",
      "         0.0318, -0.0078,  0.0246,  0.0738, -0.0138, -0.0257, -0.0100,  0.1071,\n",
      "        -0.0313,  0.0428,  0.0653,  0.1850, -0.0467,  0.0341, -0.1505, -0.1017,\n",
      "         0.1310, -0.1134, -0.0503,  0.1108, -0.0271, -0.0086, -0.0414, -0.0951,\n",
      "         0.0876, -0.0573,  0.1169, -0.0305,  0.0528,  0.0202, -0.0349, -0.0190,\n",
      "         0.0291,  0.1131, -0.0913, -0.1096, -0.0182,  0.0200,  0.2308, -0.0317,\n",
      "        -0.0843,  0.0931,  0.1105,  0.1545, -0.1171,  0.0313, -0.1181,  0.3096,\n",
      "        -0.1541,  0.0994,  0.0948,  0.1635, -0.0836, -0.0963,  0.0993,  0.0305,\n",
      "         0.0589,  0.0615, -0.0847,  0.0325,  0.0145, -0.0310,  0.1617, -0.0714,\n",
      "         0.0280,  0.1086,  0.1843, -0.1734,  0.0447,  0.1283,  0.1478, -0.0873,\n",
      "        -0.0654,  0.0688, -0.0695,  0.0535, -0.0323, -0.0611,  0.0578, -0.0245,\n",
      "        -0.0596, -0.1169,  0.0942,  0.0717,  0.0101,  0.0070, -0.0139,  0.0354,\n",
      "         0.0965,  0.0765, -0.0417,  0.0521,  0.0214,  0.0609, -0.0798,  0.1009,\n",
      "         0.0703, -0.1515,  0.0108, -0.1594,  0.0037, -0.0049,  0.1171,  0.0757,\n",
      "        -0.0990, -0.0339, -0.0005, -0.2825, -0.2046, -0.0797, -0.0279, -0.0743,\n",
      "         0.0950, -0.0037, -0.0536, -0.0041, -0.0459, -0.1475, -0.2407, -0.2186,\n",
      "         0.0295,  0.0088, -0.0471, -0.0663, -0.0767,  0.0034, -0.0042, -0.1389,\n",
      "        -0.0477, -0.0345, -0.1194,  0.1120, -0.0230,  0.0658, -0.0482, -0.0037,\n",
      "         0.0468, -0.0971, -0.1441,  0.0507, -0.0354, -0.1686, -0.1899, -0.1533,\n",
      "        -0.2763,  0.1184,  0.1278,  0.0610,  0.0823, -0.1097,  0.0270,  0.1028,\n",
      "        -0.0496,  0.0294,  0.2576,  0.0440,  0.0458,  0.2121,  0.1608, -0.0647,\n",
      "         0.1250, -0.0848,  0.0679, -0.0015,  0.0730, -0.0363, -0.0486, -0.1547,\n",
      "         0.0188,  0.0306, -0.0099, -0.0976,  0.0955, -0.0154, -0.0222, -0.0836],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 86,  40,  92,  ..., 303, 303, 303])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T16:26:42.809193Z",
     "start_time": "2026-02-20T16:26:42.803189Z"
    }
   },
   "cell_type": "code",
   "source": "loss",
   "id": "dc1ccfafacf3984a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T16:26:48.345105Z",
     "start_time": "2026-02-20T16:26:48.254740Z"
    }
   },
   "cell_type": "code",
   "source": "logits",
   "id": "ba538c799049dfa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0568,  0.0527, -0.0047,  ..., -0.0154, -0.0222, -0.0836],\n",
       "        [ 0.0566,  0.0528, -0.0047,  ..., -0.0155, -0.0223, -0.0836],\n",
       "        [ 0.0568,  0.0528, -0.0048,  ..., -0.0154, -0.0224, -0.0836],\n",
       "        ...,\n",
       "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f3185e1a4d55813f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
